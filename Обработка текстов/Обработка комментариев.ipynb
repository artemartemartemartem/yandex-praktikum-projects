{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ARTEM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Загрузим необходимые библиотеки\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords \n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import catboost as cb\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загрузим данные, добавим Index_col=0, потому что подгружалась дополнительный столбец\n",
    "try: \n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv',index_col=0)\n",
    "except:\n",
    "    data=pd.read_csv('C:/Users/ARTEM/Desktop/Project_DS/Project Ds/toxic_comments.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Посмотрим на данные\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#Посмотрим на данные с этого среза\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создадим корпус постов\n",
    "corpus = list(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Инициализируем лемматайзер\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    y=re.sub(r\"[^'a-zA-Z ]\", ' ', text) \n",
    "    clear=\" \".join(y.split())\n",
    "    return clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Найдем правильный POS-тег\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088f68bbaf5948c290160e046c30191e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 18min 4s\n",
      "Wall time: 1h 17min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def transform(text):\n",
    "    a=[]\n",
    "    for i in nltk.word_tokenize(text):\n",
    "        b=lemmatizer.lemmatize(i, get_wordnet_pos(i))\n",
    "        a.append(b)\n",
    "    return ' '.join(a) \n",
    "\n",
    "ready=[]\n",
    "for i in tqdm(range(len(corpus))):    \n",
    "    ready.append(transform(clear_text(corpus[i])))\n",
    "data['lemm_text']=pd.Series(ready, index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D'aww He match this background colour I 'm see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I 'm really not try to edit war It 's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I ca n't make any real suggestion on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir be my hero Any chance you remember wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159446</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>And for the second time of ask when your view ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159447</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>You should be ashamed of yourself That be a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159448</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Spitzer Umm there no actual article for prosti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159449</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>And it look like it be actually you who put on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159450</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>And I really do n't think you understand I com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159292 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159446  \":::::And for the second time of asking, when ...      0   \n",
       "159447  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159448  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159449  And it looks like it was actually you who put ...      0   \n",
       "159450  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "                                                lemm_text  \n",
       "0       Explanation Why the edits make under my userna...  \n",
       "1       D'aww He match this background colour I 'm see...  \n",
       "2       Hey man I 'm really not try to edit war It 's ...  \n",
       "3       More I ca n't make any real suggestion on impr...  \n",
       "4       You sir be my hero Any chance you remember wha...  \n",
       "...                                                   ...  \n",
       "159446  And for the second time of ask when your view ...  \n",
       "159447  You should be ashamed of yourself That be a ho...  \n",
       "159448  Spitzer Umm there no actual article for prosti...  \n",
       "159449  And it look like it be actually you who put on...  \n",
       "159450  And I really do n't think you understand I com...  \n",
       "\n",
       "[159292 rows x 3 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Проверим\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Разделим данные на признак и таргет\n",
    "features = data['lemm_text']\n",
    "target = data['toxic']\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119469,)\n",
      "(39823,)\n",
      "(119469,)\n",
      "(39823,)\n"
     ]
    }
   ],
   "source": [
    "#Проверим, все ок!\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "print(target_train.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ARTEM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер мешка с учётом стоп-слов: (119469, 134044)\n",
      "Размер мешка с учётом стоп-слов: (39823, 134044)\n"
     ]
    }
   ],
   "source": [
    "#Векторизируем данные\n",
    "nltk.download('stopwords') \n",
    "stop_words = set(stopwords.words('english'))\n",
    "count_vect = TfidfVectorizer (stop_words=stop_words) \n",
    "features_train=count_vect.fit_transform(features_train)\n",
    "features_test=count_vect.transform(features_test)\n",
    "print(\"Размер мешка с учётом стоп-слов:\", features_train.shape)\n",
    "print(\"Размер мешка с учётом стоп-слов:\", features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод\n",
    "- Создан корпус постов и преобразован столбец 'text' в список текстов\n",
    "- Тексты переведены в стандартный для Python формат: кодировку Unicode U\n",
    "- Тексты лемматизированы с помощью WordNetLemmatizer и очищены\n",
    "- Произведена векторизация текстов с помощью CountVectorizer, произведены очистка от ненужных слов\n",
    "- Данные разбиты на обучающую и тестовую выборки 1:4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обучим логистическую регрессию\n",
    "model = LogisticRegression(random_state=12345,solver='liblinear')\n",
    "param_search={'penalty':[\"l1\",\"l2\"],'max_iter':[100,1000,2500,5000]}\n",
    "gsearch = GridSearchCV(estimator=model, cv=3, param_grid=param_search, scoring = 'f1')\n",
    "gsearch.fit(features_train, target_train)\n",
    "best_score = gsearch.best_score_\n",
    "best_model = gsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7491350642998696\n",
      "LogisticRegression(penalty='l1', random_state=12345, solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "print(best_score)\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обучим случайный лес\n",
    "model = RandomForestClassifier(random_state=12345)\n",
    "param_search = { \n",
    "    'n_estimators': [3, 10, 30],\n",
    "    'max_depth' : [i for i in range(1,5)]\n",
    "}\n",
    "gsearch = GridSearchCV(estimator=model, cv=3, param_grid=param_search, scoring = 'f1')\n",
    "gsearch.fit(features_train, target_train)\n",
    "best_score = gsearch.best_score_\n",
    "best_model = gsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006585175425755136\n",
      "RandomForestClassifier(max_depth=4, n_estimators=3, random_state=12345)\n"
     ]
    }
   ],
   "source": [
    "print(best_score)\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3557218\ttotal: 232ms\tremaining: 929ms\n",
      "1:\tlearn: 0.2939784\ttotal: 298ms\tremaining: 447ms\n",
      "2:\tlearn: 0.2743006\ttotal: 363ms\tremaining: 242ms\n",
      "3:\tlearn: 0.2661671\ttotal: 429ms\tremaining: 107ms\n",
      "4:\tlearn: 0.2602970\ttotal: 497ms\tremaining: 0us\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3582325\ttotal: 69.5ms\tremaining: 278ms\n",
      "1:\tlearn: 0.2930260\ttotal: 140ms\tremaining: 209ms\n",
      "2:\tlearn: 0.2745811\ttotal: 209ms\tremaining: 140ms\n",
      "3:\tlearn: 0.2669147\ttotal: 282ms\tremaining: 70.4ms\n",
      "4:\tlearn: 0.2616268\ttotal: 355ms\tremaining: 0us\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3554457\ttotal: 70.5ms\tremaining: 282ms\n",
      "1:\tlearn: 0.2934114\ttotal: 142ms\tremaining: 213ms\n",
      "2:\tlearn: 0.2733860\ttotal: 212ms\tremaining: 141ms\n",
      "3:\tlearn: 0.2654239\ttotal: 284ms\tremaining: 70.9ms\n",
      "4:\tlearn: 0.2602592\ttotal: 356ms\tremaining: 0us\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3547895\ttotal: 326ms\tremaining: 1.3s\n",
      "1:\tlearn: 0.2678432\ttotal: 682ms\tremaining: 1.02s\n",
      "2:\tlearn: 0.2417908\ttotal: 1.04s\tremaining: 691ms\n",
      "3:\tlearn: 0.2258161\ttotal: 1.39s\tremaining: 347ms\n",
      "4:\tlearn: 0.2179866\ttotal: 1.73s\tremaining: 0us\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3497577\ttotal: 330ms\tremaining: 1.32s\n",
      "1:\tlearn: 0.2640455\ttotal: 703ms\tremaining: 1.05s\n",
      "2:\tlearn: 0.2392151\ttotal: 1.06s\tremaining: 709ms\n",
      "3:\tlearn: 0.2277174\ttotal: 1.44s\tremaining: 359ms\n",
      "4:\tlearn: 0.2174804\ttotal: 1.8s\tremaining: 0us\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3418010\ttotal: 316ms\tremaining: 1.26s\n",
      "1:\tlearn: 0.2597248\ttotal: 682ms\tremaining: 1.02s\n",
      "2:\tlearn: 0.2382676\ttotal: 1.04s\tremaining: 692ms\n",
      "3:\tlearn: 0.2248518\ttotal: 1.39s\tremaining: 348ms\n",
      "4:\tlearn: 0.2168510\ttotal: 1.76s\tremaining: 0us\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3442542\ttotal: 470ms\tremaining: 1.88s\n",
      "1:\tlearn: 0.2625574\ttotal: 896ms\tremaining: 1.34s\n",
      "2:\tlearn: 0.2401192\ttotal: 1.33s\tremaining: 885ms\n",
      "3:\tlearn: 0.2291030\ttotal: 1.76s\tremaining: 439ms\n",
      "4:\tlearn: 0.2179874\ttotal: 2.19s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "#Обучем кэтбуст\n",
    "model = cb.CatBoostClassifier(iterations=5,random_state=12345)\n",
    "#Определяю словарь с набором параметров\n",
    "param_search = {'depth': [1, 5]}\n",
    "\n",
    "gsearch = GridSearchCV(estimator=model, cv=3,param_grid=param_search, scoring ='f1')\n",
    "gsearch.fit(features_train, target_train)\n",
    "best_score = gsearch.best_score_\n",
    "best_model = gsearch.best_estimator_\n",
    "best_param=gsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5340327192018443\n",
      "<catboost.core.CatBoostClassifier object at 0x0000014E6DBA6BB0>\n",
      "{'depth': 5}\n"
     ]
    }
   ],
   "source": [
    "print(best_score)\n",
    "print(best_model)\n",
    "print(gsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ARTEM\\AppData\\Local\\Temp\\ipykernel_23500\\464169241.py:8: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  result.style.hide_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6695b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_6695b_level0_col0\" class=\"col_heading level0 col0\" >Модель</th>\n",
       "      <th id=\"T_6695b_level0_col1\" class=\"col_heading level0 col1\" >f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_6695b_row0_col0\" class=\"data row0 col0\" >Логистическая регрессия</td>\n",
       "      <td id=\"T_6695b_row0_col1\" class=\"data row0 col1\" >0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6695b_row1_col0\" class=\"data row1 col0\" >Случайный лес</td>\n",
       "      <td id=\"T_6695b_row1_col1\" class=\"data row1 col1\" >0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6695b_row2_col0\" class=\"data row2 col0\" >Catboost</td>\n",
       "      <td id=\"T_6695b_row2_col1\" class=\"data row2 col1\" >0.530000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14e70e864c0>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Сведем результаты\n",
    "result = {'Модель': ['Логистическая регрессия', 'Случайный лес', 'Catboost'],\n",
    "                  'f1-score': [0.75, 0.0006, 0.53]\n",
    "                 }\n",
    "result=pd.DataFrame(result)\n",
    "\n",
    "\n",
    "result.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод\n",
    "Применили 3 модели из них наиболее оптимальным выглядит логистическая регрессия. Были попытки в catboost добавить дополнительные параметры, но, к сожалению, слишком сильно увеличивается время работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7780744905130006\n"
     ]
    }
   ],
   "source": [
    "#Проверим логистическую регрессию на тестовой выборке\n",
    "model = LogisticRegression(penalty='l1', random_state=12345, solver='liblinear')\n",
    "model = model.fit(features_train, target_train)\n",
    "predict = model.predict(features_test)\n",
    "print(f1_score(target_test,predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод\n",
    "По результатам теста значение метрики F1 Score не меньше 0.75, мы в целевых значениях!"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 86,
    "start_time": "2023-08-05T11:18:51.254Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
